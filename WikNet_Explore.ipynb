{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better aligned Wiki dataset : WikNet\n",
    "http://ssli.ee.washington.edu/tial/projects/simplification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tengi/anaconda3/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import sys\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, './subword_nmt')\n",
    "from subword_nmt import learn_bpe\n",
    "from subword_nmt import apply_bpe\n",
    "sys.path.insert(0, './nmt/nmt')\n",
    "\n",
    "# sys.path.insert(0, '/Users/tengi/insight-project/lib/pwkp_dataset_parser')\n",
    "# from pwkp_parser import parse_pwkp_file\n",
    "# download the princeton wordnet, once run it once\n",
    "# nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "%matplotlib inline\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the scores\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/WikNet_word_pairs.txt -P ./data/WikNet/\n",
    "#download annotations\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/annotations.txt -P ./data/WikNet/\n",
    "#download good alignment\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good-0.67.txt -P ./data/WikNet/\n",
    "#download partial alignment\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good_partial-0.53.txt -P ./data/WikNet/\n",
    "#downad uncategorized\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-remaining-0.45.txt -P ./data/WikNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiknet_good_file = 'data/WikNet/aligned-good-0.67.txt'\n",
    "wiknet_partial_file = 'data/WikNet/aligned-good_partial-0.53.txt'\n",
    "wiknet_good_prefix = 'data/WikNet/good'\n",
    "wiknet_partial_prefix = 'data/WikNet/partial'\n",
    "\n",
    "wiknet_good_partial_prefix = 'data/WikNet/good_partial'\n",
    "\n",
    "\n",
    "translations = []\n",
    "# generate complex-simple paris in translations\n",
    "def parse_wiknet_file(src_file):\n",
    "    translations = []\n",
    "    scores = []\n",
    "    with open(src_file) as wiknet_file:\n",
    "            for line in wiknet_file:\n",
    "                triplet = line.split(\"\\t\")\n",
    "                translations.append([triplet[0].strip(), triplet[1].strip()])\n",
    "                scores.append(triplet[0].strip())               \n",
    "\n",
    "    return (translations, scores);\n",
    "def split_train_val_test(translations, src_prefix):\n",
    "    np.random.shuffle(translations)\n",
    "    complex_tok_str = [pair[0] for pair in translations]\n",
    "    simple_tok_str = [pair[1] for pair in translations]\n",
    "    \n",
    "    corpus_file = src_prefix+'.all.tok'\n",
    "    \n",
    "    complex_file = src_prefix+'.tok.complex'\n",
    "    simple_file = src_prefix+'.tok.simple'\n",
    "    \n",
    "    complex_train_file = src_prefix+'.train.tok.complex'\n",
    "    simple_train_file = src_prefix+'.train.tok.simple'\n",
    "\n",
    "    complex_val_file = src_prefix+'.val.tok.complex'\n",
    "    simple_val_file = src_prefix+'.val.tok.simple'\n",
    "\n",
    "    complex_test_file = src_prefix+'.test.tok.complex'\n",
    "    simple_test_file = src_prefix+'.test.tok.simple'\n",
    "\n",
    "    val_size = 300\n",
    "    test_size = 300\n",
    "\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        for i in range(len(complex_tok_str)):\n",
    "            f.write(complex_tok_str[i] + '\\n')\n",
    "            f.write(simple_tok_str[i] + '\\n\\n')\n",
    "\n",
    "\n",
    "    with open(complex_file, 'w') as f:\n",
    "        for sent in complex_tok_str:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_file, 'w') as f:\n",
    "        for sent in simple_tok_str:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_train_file, 'w') as f:\n",
    "        for sent in complex_tok_str[:-(val_size+test_size)]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_train_file, 'w') as f:\n",
    "        for sent in simple_tok_str[:-(val_size+test_size)]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_val_file, 'w') as f:\n",
    "        for sent in complex_tok_str[-(val_size+test_size):-test_size]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_val_file, 'w') as f:\n",
    "        for sent in simple_tok_str[-(val_size+test_size):-test_size]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_test_file, 'w') as f:\n",
    "        for sent in complex_tok_str[-test_size:]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_test_file, 'w') as f:\n",
    "        for sent in simple_tok_str[-test_size:]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "\n",
    "good_translations, _= parse_wiknet_file(wiknet_good_file)\n",
    "split_train_val_test(good_translations, wiknet_good_prefix)\n",
    "\n",
    "partial_translations, _= parse_wiknet_file(wiknet_partial_file)\n",
    "split_train_val_test(partial_translations, wiknet_partial_prefix)\n",
    "\n",
    "split_train_val_test(good_translations+partial_translations, wiknet_good_partial_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a byte-pair-encoding (bpe) vocabulary using 10,000 merge operations\n",
    "!./subword_nmt/learn_bpe.py -s 10000 < data/WikNet/good_partial.all.tok > data/WikNet/codes.bpe\n",
    "\n",
    "# Apply the vocabulary to the training file\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.all.tok > data/WikNet/good_partial.tok.bpe\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.tok.complex > data/WikNet/good_partial.tok.bpe.complex\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.tok.simple > data/WikNet/good_partial.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.train.tok.complex > data/WikNet/good_partial.train.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.train.tok.simple > data/WikNet/good_partial.train.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.val.tok.complex > data/WikNet/good_partial.val.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.val.tok.simple > data/WikNet/good_partial.val.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.test.tok.complex > data/WikNet/good_partial.test.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.test.tok.simple > data/WikNet/good_partial.test.tok.bpe.simple\n",
    "\n",
    "!cat data/WikNet/good_partial.tok.bpe | ./subword_nmt/get_vocab.py > data/WikNet/good_partial.vocab.tok.bpe\n",
    "!cat data/WikNet/good_partial.tok.bpe.complex | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.bpe.complex\n",
    "!cat data/WikNet/good_partial.tok.bpe.simple | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.bpe.simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer()\n",
    "good_partial_translations = good_translations+partial_translations\n",
    "\n",
    "complex_sentences = [pair[0] for pair in good_partial_translations]\n",
    "simple_sentences = [pair[1] for pair in good_partial_translations]\n",
    "\n",
    "complex_tok_str = [tokenizer.tokenize(sent, return_str = True) for sent in complex_sentences]\n",
    "simple_tok_str = [tokenizer.tokenize(sent, return_str = True) for sent in simple_sentences] \n",
    "\n",
    "complex_tok_words = [word for sent in complex_tok_str for word in sent.split()]\n",
    "simple_tok_words = [word for sent in simple_tok_str for word in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_fdist = nltk.FreqDist(complex_tok_words)\n",
    "simple_fdist = nltk.FreqDist(simple_tok_words)\n",
    "\n",
    "complex_chars = sum([len(word) for word in complex_fdist.keys()])\n",
    "simple_chars = sum([len(word) for word in simple_fdist.keys()])\n",
    "\n",
    "df = pd.DataFrame({'name': ['complex', 'simple'],\n",
    "                   '#tokens/sentence': [complex_fdist.N()/len(complex_sentences), simple_fdist.N()/len(simple_sentences)],\n",
    "                   '#chars/tokens': [complex_chars/complex_fdist.B(), simple_chars/simple_fdist.B()],\n",
    "                   '#unique tokens': [complex_fdist.B(), simple_fdist.B()]})\n",
    "df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(complex_tok_words+simple_tok_words)\n",
    "print(\"Number of tokens before subword segmentation: {}\".format(fdist.B()))\n",
    "\n",
    "text_file = open(r'data/WikNet/good_partial.tok.bpe',\"r\")\n",
    "p = text_file.read()\n",
    "words = word_tokenize(p)\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Number of tokens after subword segmentation: {}\".format(fdist.B()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
